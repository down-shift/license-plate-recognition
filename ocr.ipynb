{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport os\nimport gc\nimport json\nimport glob\nimport re\nimport PIL\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom matplotlib.path import Path\nfrom torchvision import models\nfrom torchvision import transforms\nfrom PIL import Image, ImageDraw\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torchmetrics import CharErrorRate as CER\nfrom IPython.display import Image as Img","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-16T20:54:40.823861Z","iopub.execute_input":"2022-07-16T20:54:40.824533Z","iopub.status.idle":"2022-07-16T20:54:41.378698Z","shell.execute_reply.started":"2022-07-16T20:54:40.824489Z","shell.execute_reply":"2022-07-16T20:54:41.377727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_json(file):\n    with open(file, 'r') as f:\n        return json.load(f)","metadata":{"execution":{"iopub.status.busy":"2022-07-16T18:27:09.404996Z","iopub.execute_input":"2022-07-16T18:27:09.405543Z","iopub.status.idle":"2022-07-16T18:27:09.417210Z","shell.execute_reply.started":"2022-07-16T18:27:09.405512Z","shell.execute_reply":"2022-07-16T18:27:09.415760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = '../input/nomeroff-russian-license-plates/autoriaNumberplateOcrRu-2021-09-01'\nOCR_MODEL_PATH = './models/model-7-0.9156.ckpt'\nALPHABET = '0123456789ABEKMHOPCTYX'\nTRAIN_SIZE = 0.9\nBATCH_SIZE_OCR = 16\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-07-16T20:57:54.374769Z","iopub.execute_input":"2022-07-16T20:57:54.375143Z","iopub.status.idle":"2022-07-16T20:57:54.383690Z","shell.execute_reply.started":"2022-07-16T20:57:54.375113Z","shell.execute_reply":"2022-07-16T20:57:54.382688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Загружаем метки к изображениям","metadata":{}},{"cell_type":"code","source":"def get_annot(file, dir):\n    return load_json(PATH + f'/{dir}/ann/' + file[:-3] + 'json')['description']","metadata":{"execution":{"iopub.status.busy":"2022-07-16T20:58:51.249465Z","iopub.execute_input":"2022-07-16T20:58:51.249825Z","iopub.status.idle":"2022-07-16T20:58:51.255728Z","shell.execute_reply.started":"2022-07-16T20:58:51.249796Z","shell.execute_reply":"2022-07-16T20:58:51.254742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels = pd.DataFrame([[f, get_annot(f, 'train')] for f in os.listdir(PATH + '/train/img')], columns=['filename', 'label']).to_dict('records')\nval_labels = pd.DataFrame([[f, get_annot(f, 'val')] for f in os.listdir(PATH + '/val/img')], columns=['filename', 'label']).to_dict('records')\ntrain_labels[:10]","metadata":{"execution":{"iopub.status.busy":"2022-07-16T20:58:52.063916Z","iopub.execute_input":"2022-07-16T20:58:52.064570Z","iopub.status.idle":"2022-07-16T21:00:21.955063Z","shell.execute_reply.started":"2022-07-16T20:58:52.064534Z","shell.execute_reply":"2022-07-16T21:00:21.954095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Определяем класс датасета и трансформации изображений","metadata":{}},{"cell_type":"code","source":"class OCRDataset(Dataset):\n    def __init__(self, marks, img_folder, tokenizer, transforms=None):\n        self.img_paths = []\n        self.texts = []\n        for item in marks:\n            self.img_paths.append(os.path.join(PATH + f'/{img_folder}/img/', item['filename']))\n            self.texts.append(item['label'])\n            \n        self.enc_texts = tokenizer.encode(self.texts)\n        self.img_folder = PATH + f'/{img_folder}/img/'\n        self.transforms = transforms\n        \n        \n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        text = self.texts[idx]\n        enc_text = torch.LongTensor(self.enc_texts[idx])\n        image = cv2.imread(img_path)\n        if self.transforms is not None:\n            image = self.transforms(image)\n            \n        return image, text, enc_text\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    \nclass Resize(object):\n    def __init__(self, size=(250, 50)):\n        self.size = size\n\n    def __call__(self, img):\n        w_from, h_from = img.shape[1], img.shape[0]\n        w_to, h_to = self.size\n        \n        # Сделаем разную интерполяцию при увеличении и уменьшении\n        # Если увеличиваем картинку, меняем интерполяцию\n        interpolation = cv2.INTER_AREA\n        if w_to > w_from:\n            interpolation = cv2.INTER_CUBIC\n        \n        img = cv2.resize(img, dsize=self.size, interpolation=interpolation)\n        return img\n\n\nclass Normalize:\n    def __call__(self, img):\n        img = img.astype(np.float32) / 255\n        return img\n\n\ndef collate_fn(batch):\n    images, texts, enc_texts = zip(*batch)\n    images = torch.stack(images, 0)\n    text_lens = torch.LongTensor([len(text) for text in texts])\n    enc_pad_texts = pad_sequence(enc_texts, batch_first=True, padding_value=0)\n    return images, texts, enc_pad_texts, text_lens","metadata":{"execution":{"iopub.status.busy":"2022-07-16T21:00:21.957231Z","iopub.execute_input":"2022-07-16T21:00:21.957607Z","iopub.status.idle":"2022-07-16T21:00:21.973552Z","shell.execute_reply.started":"2022-07-16T21:00:21.957553Z","shell.execute_reply":"2022-07-16T21:00:21.972547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Определяем токенайзер","metadata":{}},{"cell_type":"code","source":"OOV_TOKEN = '<OOV>'\nCTC_BLANK = '<BLANK>'\n\n\ndef get_char_map(alphabet):\n    char_map = {value: idx + 2 for (idx, value) in enumerate(alphabet)}\n    char_map[CTC_BLANK] = 0\n    char_map[OOV_TOKEN] = 1\n    return char_map\n\n\nclass Tokenizer:\n    def __init__(self, alphabet):\n        self.char_map = get_char_map(alphabet)\n        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n\n    def encode(self, word_list):\n        enc_words = []\n        for word in word_list:\n            enc_words.append(\n                [self.char_map[char] if char in self.char_map\n                 else self.char_map[OOV_TOKEN]\n                 for char in word]\n            )\n        return enc_words\n\n    def get_num_chars(self):\n        return len(self.char_map)\n\n    def decode(self, enc_word_list):\n        dec_words = []\n        for word in enc_word_list:\n            word_chars = ''\n            for idx, char_enc in enumerate(word):\n                # пропустить повторяющиеся/пустые символы\n                if (\n                    char_enc != self.char_map[OOV_TOKEN]\n                    and char_enc != self.char_map[CTC_BLANK]\n                    and not (idx > 0 and char_enc == word[idx - 1])\n                ):\n                    word_chars += self.rev_char_map[char_enc]\n            dec_words.append(word_chars)\n        return dec_words","metadata":{"execution":{"iopub.status.busy":"2022-07-16T21:00:21.975034Z","iopub.execute_input":"2022-07-16T21:00:21.975895Z","iopub.status.idle":"2022-07-16T21:00:21.989253Z","shell.execute_reply.started":"2022-07-16T21:00:21.975856Z","shell.execute_reply":"2022-07-16T21:00:21.988296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Инициализируем токенайзер, трансформации и датасет","metadata":{}},{"cell_type":"code","source":"ocr_transforms = transforms.Compose([\n    Resize(size=(250, 50)),\n    Normalize(),\n    transforms.ToTensor()\n])\n\ntokenizer = Tokenizer(ALPHABET)\n\ntrain_ocr_dataset = OCRDataset(\n    marks=train_labels, \n    img_folder='train', \n    tokenizer=tokenizer,\n    transforms=ocr_transforms\n)\nval_ocr_dataset = OCRDataset(\n    marks=val_labels,\n    img_folder='val', \n    tokenizer=tokenizer,\n    transforms=ocr_transforms\n)\n\ntrain_loader = DataLoader(\n    train_ocr_dataset, \n    batch_size=BATCH_SIZE_OCR, \n    drop_last=True,\n    num_workers=2,\n    collate_fn=collate_fn,\n    timeout=0,\n    shuffle=True \n)\nval_loader = DataLoader(\n    val_ocr_dataset, \n    batch_size=BATCH_SIZE_OCR, \n    drop_last=False,\n    num_workers=2,\n    collate_fn=collate_fn, \n    timeout=0,\n)\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-07-16T21:00:21.992226Z","iopub.execute_input":"2022-07-16T21:00:21.992992Z","iopub.status.idle":"2022-07-16T21:00:22.498947Z","shell.execute_reply.started":"2022-07-16T21:00:21.992941Z","shell.execute_reply":"2022-07-16T21:00:22.498025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(train_ocr_dataset[0]['img'].permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2022-07-16T17:36:04.420390Z","iopub.execute_input":"2022-07-16T17:36:04.421126Z","iopub.status.idle":"2022-07-16T17:36:04.598178Z","shell.execute_reply.started":"2022-07-16T17:36:04.421086Z","shell.execute_reply":"2022-07-16T17:36:04.597123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Определяем класс модели","metadata":{}},{"cell_type":"code","source":"def get_resnet34_backbone():\n    m = models.resnet34(pretrained=True)\n    input_conv = nn.Conv2d(3, 64, 7, 1, 3)\n    blocks = [input_conv, m.bn1, m.relu,\n              m.maxpool, m.layer1, m.layer2, m.layer3]\n    return nn.Sequential(*blocks)\n\n\nclass BiLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, dropout=0.1):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size, hidden_size, num_layers,\n            dropout=dropout, batch_first=True, bidirectional=True)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        return out\n\n\nclass CRNN(nn.Module):\n    def __init__(\n        self, number_class_symbols, time_feature_count=256, lstm_hidden=256,\n        lstm_len=2,\n    ):\n        super().__init__()\n        self.feature_extractor = get_resnet34_backbone()\n        self.avg_pool = nn.AdaptiveAvgPool2d(\n            (time_feature_count, time_feature_count))\n        self.bilstm = BiLSTM(time_feature_count, lstm_hidden, lstm_len)\n        self.classifier = nn.Sequential(\n            nn.Linear(lstm_hidden * 2, time_feature_count),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(time_feature_count, number_class_symbols)\n        )\n\n    def forward(self, x):\n        x = self.feature_extractor(x)\n        b, c, h, w = x.size()\n        x = x.view(b, c * h, w)\n        x = self.avg_pool(x)\n        x = x.transpose(1, 2)\n        x = self.bilstm(x)\n        x = self.classifier(x)\n        x = nn.functional.log_softmax(x, dim=2).permute(1, 0, 2)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-07-16T21:00:22.500397Z","iopub.execute_input":"2022-07-16T21:00:22.500747Z","iopub.status.idle":"2022-07-16T21:00:22.514280Z","shell.execute_reply.started":"2022-07-16T21:00:22.500712Z","shell.execute_reply":"2022-07-16T21:00:22.512984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Подготовка к обучению","metadata":{}},{"cell_type":"code","source":"model = CRNN(number_class_symbols=tokenizer.get_num_chars())\nmodel.to(device)\n\ncriterion = torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.001,\n                              weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer=optimizer, mode='max', factor=0.5, patience=15)","metadata":{"execution":{"iopub.status.busy":"2022-07-16T21:00:22.515884Z","iopub.execute_input":"2022-07-16T21:00:22.516608Z","iopub.status.idle":"2022-07-16T21:00:23.012204Z","shell.execute_reply.started":"2022-07-16T21:00:22.516573Z","shell.execute_reply":"2022-07-16T21:00:23.011124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter:\n#     Вычисляет и хранит среднее значение\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2022-07-16T21:00:23.013799Z","iopub.execute_input":"2022-07-16T21:00:23.014188Z","iopub.status.idle":"2022-07-16T21:00:23.022100Z","shell.execute_reply.started":"2022-07-16T21:00:23.014152Z","shell.execute_reply":"2022-07-16T21:00:23.021043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_accuracy(y_true, y_pred):\n    scores = []\n    for true, pred in zip(y_true, y_pred):\n        scores.append(true == pred)\n    avg_score = np.mean(scores)\n    return avg_score\n\n\ndef predict(images, model, tokenizer, device):\n    model.eval()\n    images = images.to(device)\n    with torch.no_grad():\n        output = model(images)\n    pred = torch.argmax(output.detach().cpu(), -1).permute(1, 0).numpy()\n    text_preds = tokenizer.decode(pred)\n    return text_preds\n\n\ndef val_loop(data_loader, model, tokenizer, device):\n    acc_avg = AverageMeter()\n    for images, texts, _, _ in tqdm(data_loader):\n        batch_size = len(texts)\n        text_preds = predict(images, model, tokenizer, device)\n        acc_avg.update(get_accuracy(texts, text_preds), batch_size)\n    print(f'Validation, acc: {acc_avg.avg:.4f}\\n')\n    return acc_avg.avg\n\n\ndef train_loop(data_loader, model, criterion, optimizer, epoch):\n    loss_avg = AverageMeter()\n    model.train()\n    for images, texts, enc_pad_texts, text_lens in tqdm(data_loader):\n        model.zero_grad()\n        images = images.to(device)\n        batch_size = len(texts)\n        output = model(images)\n        output_lenghts = torch.full(\n            size=(output.size(1),),\n            fill_value=output.size(0),\n            dtype=torch.long\n        )\n        loss = criterion(output, enc_pad_texts, output_lenghts, text_lens)\n        loss_avg.update(loss.item(), batch_size)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n        optimizer.step()\n    for param_group in optimizer.param_groups:\n        lr = param_group['lr']\n    print(f'\\nEpoch {epoch}, Loss: {loss_avg.avg:.5f}, LR: {lr:.7f}')\n    return loss_avg.avg","metadata":{"execution":{"iopub.status.busy":"2022-07-16T21:00:23.023534Z","iopub.execute_input":"2022-07-16T21:00:23.024125Z","iopub.status.idle":"2022-07-16T21:00:23.039366Z","shell.execute_reply.started":"2022-07-16T21:00:23.024087Z","shell.execute_reply":"2022-07-16T21:00:23.038462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, optimizer, scheduler, train_loader, val_loader, epochs=10):\n    best_acc = -np.inf\n    os.makedirs('models', exist_ok=True)\n    acc_avg = val_loop(val_loader, model, tokenizer, device)\n    for epoch in range(epochs):\n        print(f'Epoch {epoch} started')\n        loss_avg = train_loop(train_loader, model, criterion, optimizer, epoch)\n        acc_avg = val_loop(val_loader, model, tokenizer, device)\n        scheduler.step(acc_avg)\n        if acc_avg > best_acc:\n            best_acc = acc_avg\n            model_save_path = os.path.join(\n                'models', f'model-{epoch}-{acc_avg:.4f}.ckpt')\n            torch.save(model.state_dict(), model_save_path)\n            print('Model weights saved')","metadata":{"execution":{"iopub.status.busy":"2022-07-16T21:00:23.041620Z","iopub.execute_input":"2022-07-16T21:00:23.043395Z","iopub.status.idle":"2022-07-16T21:00:23.053314Z","shell.execute_reply.started":"2022-07-16T21:00:23.043265Z","shell.execute_reply":"2022-07-16T21:00:23.052230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Запуск обучения","metadata":{}},{"cell_type":"code","source":"train(model, optimizer, scheduler, train_loader, val_loader, epochs=10)","metadata":{"execution":{"iopub.status.busy":"2022-07-16T18:46:06.339257Z","iopub.execute_input":"2022-07-16T18:46:06.339600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Определяем класс для предсказания","metadata":{}},{"cell_type":"code","source":"class InferenceTransform:\n    def __init__(self):\n        self.transforms = ocr_transforms\n\n    def __call__(self, images):\n        transformed_images = []\n        for image in images:\n            image = self.transforms(image)\n            transformed_images.append(image)\n        transformed_tensor = torch.stack(transformed_images, 0)\n        return transformed_tensor\n\n\nclass Predictor:\n    def __init__(self, model_path, device='cuda'):\n        self.tokenizer = Tokenizer(ALPHABET)\n        self.device = torch.device(device)\n        # load model\n        self.model = CRNN(number_class_symbols=self.tokenizer.get_num_chars())\n        self.model.load_state_dict(torch.load(model_path))\n        self.model.to(self.device)\n\n        self.transforms = InferenceTransform()\n\n    def __call__(self, images):\n        if isinstance(images, (list, tuple)):\n            one_image = False\n        elif isinstance(images, np.ndarray):\n            images = [images]\n            one_image = True\n        else:\n            raise Exception(f\"Input must contain np.ndarray, \"\n                            f\"tuple or list, found {type(images)}.\")\n\n        images = self.transforms(images)\n        pred = predict(images, self.model, self.tokenizer, self.device)\n\n        if one_image:\n            return pred[0]\n        else:\n            return pred","metadata":{"execution":{"iopub.status.busy":"2022-07-16T21:12:31.505821Z","iopub.execute_input":"2022-07-16T21:12:31.506192Z","iopub.status.idle":"2022-07-16T21:12:31.517446Z","shell.execute_reply.started":"2022-07-16T21:12:31.506161Z","shell.execute_reply":"2022-07-16T21:12:31.516265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Подсчет метрики Char Error Rate \n### Результат: 0.01","metadata":{}},{"cell_type":"code","source":"predictor = Predictor(OCR_MODEL_PATH)\npred_json = {}\ny_true, y_pred = [], []\n\nfor val_img in val_labels:\n    img = cv2.imread(PATH + '/val/img/' + val_img['filename'])\n    pred = predictor(img)\n    pred_json[val_img['filename']] = pred\n    y_true.append(val_img['label'])\n    y_pred.append(pred)\n#     Можно заскомментировать, чтобы не выводились изображения и предсказания\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.show()\n    print('Prediction: ', predictor(img))\n    print('True: ', val_img['label'])\n    print()\n    \nCER()(y_pred, y_true)","metadata":{"execution":{"iopub.status.busy":"2022-07-16T21:12:43.543006Z","iopub.execute_input":"2022-07-16T21:12:43.543686Z","iopub.status.idle":"2022-07-16T21:15:44.222367Z","shell.execute_reply.started":"2022-07-16T21:12:43.543648Z","shell.execute_reply":"2022-07-16T21:15:44.221231Z"},"trusted":true},"execution_count":null,"outputs":[]}]}